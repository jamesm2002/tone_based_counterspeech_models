{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "1xtfRTBnjHDr4PPHKETbzaGA-KbXCm-5h",
      "authorship_tag": "ABX9TyPbjMjPv2EEnvaV3LYFRc8+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamesm2002/tone_based_counterspeech_models/blob/main/modelTestingandComparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Testing and Comparison\n",
        "\n",
        "This is used to read in both models and generate counterspeech outputs. It is also used to generate NLP metrics"
      ],
      "metadata": {
        "id": "qxUZ29zpL8e0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmHQdRZSK3VI"
      },
      "outputs": [],
      "source": [
        "!pip install transformers sentence-transformers better_profanity torch datasets accelerate\n",
        "!pip install torch nltk rouge-score transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# all imports needed\n",
        "import torch\n",
        "import random\n",
        "import re\n",
        "import pandas as pd\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, T5Tokenizer, T5ForConditionalGeneration\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import pipeline\n"
      ],
      "metadata": {
        "id": "oxtWqtR4K8TJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# load fine-tuned counter-speech model\n",
        "model_path = \"model/t5-counterspeech-tone-model\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_path).to(device)\n",
        "\n",
        "# load LLaMA 2 model & tokenizer hugging face token is required\n",
        "HF_TOKEN = \"\"\n",
        "llama_model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "llama_tokenizer = AutoTokenizer.from_pretrained(llama_model_name, use_auth_token=HF_TOKEN)\n",
        "llama_model = AutoModelForCausalLM.from_pretrained(llama_model_name, torch_dtype=torch.float16, device_map=\"auto\", use_auth_token=HF_TOKEN)\n",
        "\n",
        "# load NLP utilities\n",
        "similarity_model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n",
        "sentiment_model = pipeline(\"sentiment-analysis\", device=0 if torch.cuda.is_available() else -1)\n",
        "\n",
        "print(\"Models Loaded Successfully!\")\n"
      ],
      "metadata": {
        "id": "kbvU2_fxK-5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from better_profanity import profanity\n",
        "\n",
        "# function to remove numbers from hate speech\n",
        "def clean_hate_speech(text):\n",
        "    return re.sub(r\"^\\d+\\\\.\\\\s*\", \"\", text)\n",
        "\n",
        "# function to check if response is clean\n",
        "# excludes gay as this is not inherently offensive if used in the right context\n",
        "def is_clean(response):\n",
        "    words_to_exclude = {\"gay\", \"gays\"}\n",
        "    words_in_text = set(response.lower().split())\n",
        "    return not any(word for word in words_in_text if word not in words_to_exclude and profanity.contains_profanity(word))\n",
        "\n",
        "# function to calculate response relevance\n",
        "def contains_relevant_terms(hate_speech, response):\n",
        "    hate_speech_embedding = similarity_model.encode(hate_speech, convert_to_tensor=True)\n",
        "    response_embedding = similarity_model.encode(response, convert_to_tensor=True)\n",
        "    cosine_similarity = util.pytorch_cos_sim(hate_speech_embedding, response_embedding)[0][0]\n",
        "    return cosine_similarity.item()\n",
        "\n",
        "# function to score sentiment\n",
        "def score_sentiment(response):\n",
        "    result = sentiment_model(response)[0]\n",
        "    if result['label'] == \"POSITIVE\":\n",
        "        return 1\n",
        "    elif result['label'] == \"NEGATIVE\":\n",
        "        return 0\n",
        "    return 0.5\n",
        "\n",
        "# function to calculate n-gram repetition penalty\n",
        "def ngram_repetition_penalty(response, n=2):\n",
        "    words = response.split()\n",
        "    ngrams = [tuple(words[i:i+n]) for i in range(len(words)-n+1)]\n",
        "    ngram_counts = Counter(ngrams)\n",
        "    repetition_penalty = sum((count - 1) for count in ngram_counts.values() if count > 1)\n",
        "    return repetition_penalty * 0.1\n",
        "\n",
        "# function to select best semantic response\n",
        "def best_semantic_response(hate_speech, responses):\n",
        "    filtered_responses = [r for r in responses if is_clean(r)]\n",
        "    if not filtered_responses:\n",
        "        return {\"response\": \"No suitable response found.\", \"total_score\": 0}, []\n",
        "\n",
        "    response_scores = []\n",
        "    for response in filtered_responses:\n",
        "        repetition_score = ngram_repetition_penalty(response)\n",
        "        sentiment = score_sentiment(response) * 0.1\n",
        "        relevance_score = contains_relevant_terms(hate_speech, response) * 0.5\n",
        "        total_score = sentiment + relevance_score - repetition_score\n",
        "        response_scores.append({\n",
        "            'response': response,\n",
        "            'relevance_score': relevance_score,\n",
        "            'sentiment_score': sentiment,\n",
        "            'repetition_penalty': repetition_score,\n",
        "            'total_score': total_score\n",
        "        })\n",
        "\n",
        "    best_response = max(response_scores, key=lambda x: x['total_score'])\n",
        "    return best_response, response_scores\n",
        "\n"
      ],
      "metadata": {
        "id": "7kH0Tv1sLAox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tone instructions\n",
        "tone_instructions = {\n",
        "    \"Inquisitive\": \"Ask a thoughtful question about the statement:\",\n",
        "    \"Confrontational\": \"Strongly refute the statement with counter-evidence:\",\n",
        "    \"Empathetic\": \"Acknowledge the concern but provide a hopeful perspective:\",\n",
        "    \"Conversational\": \"Respond in a casual and friendly way:\"\n",
        "}\n",
        "\n",
        "# function to generate counter-speech using fine-tuned T5 model\n",
        "def generate_counter_speech(hate_speech, tone):\n",
        "    prompt = f\"{tone_instructions[tone]} {hate_speech}\"\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "    responses = []\n",
        "    for _ in range(3):\n",
        "        output_ids = model.generate(\n",
        "            input_ids,\n",
        "            do_sample=True,\n",
        "            max_length=150,\n",
        "            num_beams=3,\n",
        "            temperature=1.4,\n",
        "            top_p=0.85,\n",
        "            top_k=30,\n",
        "            repetition_penalty=1.2\n",
        "        )\n",
        "        response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "        responses.append(response)\n",
        "\n",
        "    best_response, response_scores = best_semantic_response(hate_speech, responses)\n",
        "    return best_response\n",
        "\n",
        "# function to generate\n",
        "def one_shot_generate_llama(hate_speech, tone):\n",
        "    instruction = tone_instructions.get(tone, \"Respond in a neutral way:\")\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Statement: \"{hate_speech}\"\n",
        "\n",
        "    Your response (in a {tone.lower()} tone, max 35 words):\n",
        "    \"\"\"\n",
        "\n",
        "    input_ids = llama_tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        llama_model.to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_ids = llama_model.generate(\n",
        "            input_ids,\n",
        "            temperature=0.8,  # keeps responses varied but relevant\n",
        "            top_p=0.85,  # keeps responses coherent\n",
        "            top_k=30,  # reduces likelihood of repeating the prompt\n",
        "            repetition_penalty=1.3,  # prevents unnecessary repetition\n",
        "            do_sample=True\n",
        "        )\n",
        "\n",
        "    response = llama_tokenizer.decode(output_ids[0], skip_special_tokens=True).strip()\n",
        "\n",
        "    if \"max 35 words):\" in response:\n",
        "        response = response.split(\"max 35 words):\")[-1].strip()\n",
        "\n",
        "    # ensure 35-word limit\n",
        "    response_words = response.split()\n",
        "    return \" \".join(response_words[:35]) + (\"...\" if len(response_words) > 35 else \"\")"
      ],
      "metadata": {
        "id": "nOMSPc3XNvqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test output for both models\n",
        "import random\n",
        "\n",
        "# define output file path\n",
        "output_file = \"/counter_speech_output.txt\"\n",
        "\n",
        "# example hate speech input\n",
        "hate_speech_example = \"Muslims should not live here because it is impossible to assimilate them.\"\n",
        "\n",
        "# list of tones to generate responses for\n",
        "tones = [\"Conversational\", \"Confrontational\", \"Inquisitive\", \"Empathetic\"]\n",
        "\n",
        "# write results to file\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as out_f:\n",
        "    out_f.write(\"Hate Speech Example Responses\\n\\n\")\n",
        "\n",
        "    for tone in tones:\n",
        "        best_response, response_scores = generate_counter_speech(hate_speech_example, tone)\n",
        "        response_llama = one_shot_generate_llama(hate_speech_example, tone)\n",
        "\n",
        "        out_f.write(f\"Tone: {tone}\\n\")\n",
        "        out_f.write(f\"Fine-tuned Model: {best_response['response']} ({best_response['total_score']:.2f})\\n\")\n",
        "        out_f.write(f\"LLaMA 2 Response: {response_llama}\\n\\n\")\n",
        "\n",
        "print(f\"Counter-speech results saved to {output_file}\")\n"
      ],
      "metadata": {
        "id": "TwpEWL7ebzwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert-score"
      ],
      "metadata": {
        "id": "pq9Qt5RbT4AF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# used to get rougue and bleu score\n",
        "\n",
        "import random\n",
        "import nltk\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "import pandas as pd\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# load dataset\n",
        "file_path = \"data/test_counterspeech.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# define available tones\n",
        "tones = [\"Empathetic\", \"Inquisitive\", \"Confrontational\", \"Conversational\"]\n",
        "\n",
        "# function to compute BLEU score\n",
        "def compute_bleu(reference, generated):\n",
        "    reference_tokens = [nltk.word_tokenize(reference.lower())]\n",
        "    generated_tokens = nltk.word_tokenize(generated.lower())\n",
        "\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    return sentence_bleu(reference_tokens, generated_tokens, smoothing_function=smoothie)\n",
        "\n",
        "# function to compute ROUGE-L score\n",
        "def compute_rouge_l(reference, generated):\n",
        "    scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
        "    scores = scorer.score(reference, generated)\n",
        "    return scores[\"rougeL\"].fmeasure\n",
        "\n",
        "# Dictionary to store scores for each tone\n",
        "results = {tone: {\"bleu_t5\": [], \"rouge_t5\": [], \"bleu_llama\": [], \"rouge_llama\": []} for tone in tones}\n",
        "\n",
        "print(\"\\n--- Running Evaluation ---\")\n",
        "\n",
        "for tone in tones:\n",
        "    print(f\"\\n🔹 Evaluating tone: {tone}\")\n",
        "    first_output_logged = False\n",
        "\n",
        "    # randomly select 100 samples\n",
        "    sampled_df = df.sample(n=100, random_state=42)\n",
        "\n",
        "    for i, row in tqdm(sampled_df.iterrows(), total=len(sampled_df), desc=f\"Processing ({tone})\", unit=\"sample\"):\n",
        "        hate_speech = row[\"HATE_SPEECH\"]\n",
        "        reference = row[\"COUNTER_NARRATIVE\"]\n",
        "\n",
        "        generated_t5 = generate_counter_speech(hate_speech, tone=tone)\n",
        "        generated_llama = one_shot_generate_llama(hate_speech, tone=tone)\n",
        "\n",
        "        if isinstance(generated_t5, dict):\n",
        "            generated_t5 = generated_t5.get(\"response\", \"\")\n",
        "\n",
        "        if not first_output_logged:\n",
        "            print(\"\\n📝 First Generated Output:\")\n",
        "            print(f\"  T5 Output Type: {type(generated_t5)}, Value: {generated_t5}\")\n",
        "            print(f\"  LLaMA Output Type: {type(generated_llama)}, Value: {generated_llama}\")\n",
        "            first_output_logged = True\n",
        "\n",
        "        if i % 25 == 0:\n",
        "            print(f\"\\n📝 Entry {i}:\")\n",
        "            print(f\"  T5 Output Type: {type(generated_t5)}, Value: {generated_t5}\")\n",
        "            print(f\"  LLaMA Output Type: {type(generated_llama)}, Value: {generated_llama}\")\n",
        "\n",
        "        # compute scores\n",
        "        results[tone][\"bleu_t5\"].append(compute_bleu(reference, generated_t5))\n",
        "        results[tone][\"rouge_t5\"].append(compute_rouge_l(reference, generated_t5))\n",
        "\n",
        "        results[tone][\"bleu_llama\"].append(compute_bleu(reference, generated_llama))\n",
        "        results[tone][\"rouge_llama\"].append(compute_rouge_l(reference, generated_llama))\n",
        "\n",
        "# compute and print average scores per tone\n",
        "overall_bleu_t5, overall_rouge_t5, overall_bleu_llama, overall_rouge_llama = [], [], [], []\n",
        "\n",
        "print(\"\\n--- Model Evaluation Results ---\")\n",
        "for tone in tones:\n",
        "    avg_bleu_t5 = sum(results[tone][\"bleu_t5\"]) / len(results[tone][\"bleu_t5\"])\n",
        "    avg_rouge_t5 = sum(results[tone][\"rouge_t5\"]) / len(results[tone][\"rouge_t5\"])\n",
        "\n",
        "    avg_bleu_llama = sum(results[tone][\"bleu_llama\"]) / len(results[tone][\"bleu_llama\"])\n",
        "    avg_rouge_llama = sum(results[tone][\"rouge_llama\"]) / len(results[tone][\"rouge_llama\"])\n",
        "\n",
        "    overall_bleu_t5.extend(results[tone][\"bleu_t5\"])\n",
        "    overall_rouge_t5.extend(results[tone][\"rouge_t5\"])\n",
        "    overall_bleu_llama.extend(results[tone][\"bleu_llama\"])\n",
        "    overall_rouge_llama.extend(results[tone][\"rouge_llama\"])\n",
        "\n",
        "    print(f\"\\n Results for Tone: {tone}\")\n",
        "    print(f\"  T5 Model - BLEU: {avg_bleu_t5:.4f}, ROUGE-L: {avg_rouge_t5:.4f}\")\n",
        "    print(f\"  LLaMA Model - BLEU: {avg_bleu_llama:.4f}, ROUGE-L: {avg_rouge_llama:.4f}\")\n",
        "\n",
        "final_bleu_t5 = sum(overall_bleu_t5) / len(overall_bleu_t5)\n",
        "final_rouge_t5 = sum(overall_rouge_t5) / len(overall_rouge_t5)\n",
        "final_bleu_llama = sum(overall_bleu_llama) / len(overall_bleu_llama)\n",
        "final_rouge_llama = sum(overall_rouge_llama) / len(overall_rouge_llama)\n",
        "\n",
        "print(\"\\nOverall Average Scores Across All Tones:\")\n",
        "print(f\"  T5 Model - BLEU: {final_bleu_t5:.4f}, ROUGE-L: {final_rouge_t5:.4f}\")\n",
        "print(f\"  LLaMA Model - BLEU: {final_bleu_llama:.4f}, ROUGE-L: {final_rouge_llama:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "QQ6-1LWP_ihf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import nltk\n",
        "from tqdm import tqdm\n",
        "from bert_score import score\n",
        "import pandas as pd\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# load dataset\n",
        "file_path = \"data/test_counterspeech.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "tones = [\"Empathetic\", \"Inquisitive\", \"Confrontational\", \"Conversational\"]\n",
        "\n",
        "# function to compute BERTScore\n",
        "def compute_bertscore(reference, generated):\n",
        "    P, R, F1 = score(\n",
        "        [generated], [reference],\n",
        "        model_type=\"microsoft/deberta-base-mnli\",\n",
        "        rescale_with_baseline=False\n",
        "    )\n",
        "    return F1.item()\n",
        "\n",
        "\n",
        "# dictionary to store BERTScores for each tone\n",
        "results = {tone: {\"bertscore_t5\": [], \"bertscore_llama\": []} for tone in tones}\n",
        "\n",
        "print(\"\\n--- Running BERTScore Evaluation ---\")\n",
        "\n",
        "for tone in tones:\n",
        "    print(f\"\\n🔹 Evaluating tone: {tone}\")\n",
        "    first_output_logged = False\n",
        "\n",
        "    sampled_df = df.sample(n=100, random_state=42)\n",
        "\n",
        "    for i, row in tqdm(sampled_df.iterrows(), total=len(sampled_df), desc=f\"Processing ({tone})\", unit=\"sample\"):\n",
        "        hate_speech = row[\"HATE_SPEECH\"]\n",
        "        reference = row[\"COUNTER_NARRATIVE\"]\n",
        "\n",
        "        generated_t5 = generate_counter_speech(hate_speech, tone=tone)\n",
        "        generated_llama = one_shot_generate_llama(hate_speech, tone=tone)\n",
        "\n",
        "        if isinstance(generated_t5, dict):\n",
        "            generated_t5 = generated_t5.get(\"response\", \"\")\n",
        "\n",
        "        if not first_output_logged:\n",
        "            print(\"\\nFirst Generated Output:\")\n",
        "            print(f\"  T5 Output Type: {type(generated_t5)}, Value: {generated_t5}\")\n",
        "            print(f\"  LLaMA Output Type: {type(generated_llama)}, Value: {generated_llama}\")\n",
        "            first_output_logged = True\n",
        "\n",
        "        if i % 25 == 0:\n",
        "            print(f\"\\n Entry {i}:\")\n",
        "            print(f\"  T5 Output Type: {type(generated_t5)}, Value: {generated_t5}\")\n",
        "            print(f\"  LLaMA Output Type: {type(generated_llama)}, Value: {generated_llama}\")\n",
        "\n",
        "        results[tone][\"bertscore_t5\"].append(compute_bertscore(reference, generated_t5))\n",
        "        results[tone][\"bertscore_llama\"].append(compute_bertscore(reference, generated_llama))\n",
        "\n",
        "overall_bertscore_t5, overall_bertscore_llama = [], []\n",
        "\n",
        "print(\"\\n--- BERTScore Evaluation Results ---\")\n",
        "for tone in tones:\n",
        "    avg_bertscore_t5 = sum(results[tone][\"bertscore_t5\"]) / len(results[tone][\"bertscore_t5\"])\n",
        "    avg_bertscore_llama = sum(results[tone][\"bertscore_llama\"]) / len(results[tone][\"bertscore_llama\"])\n",
        "\n",
        "    overall_bertscore_t5.extend(results[tone][\"bertscore_t5\"])\n",
        "    overall_bertscore_llama.extend(results[tone][\"bertscore_llama\"])\n",
        "\n",
        "    print(f\"\\n Results for Tone: {tone}\")\n",
        "    print(f\"  T5 Model - BERTScore: {avg_bertscore_t5:.4f}\")\n",
        "    print(f\"  LLaMA Model - BERTScore: {avg_bertscore_llama:.4f}\")\n",
        "\n",
        "# compute overall average across all tones\n",
        "final_bertscore_t5 = sum(overall_bertscore_t5) / len(overall_bertscore_t5)\n",
        "final_bertscore_llama = sum(overall_bertscore_llama) / len(overall_bertscore_llama)\n",
        "\n",
        "print(\"\\n Overall Average BERTScore Across All Tones:\")\n",
        "print(f\"  T5 Model - BERTScore: {final_bertscore_t5:.4f}\")\n",
        "print(f\"  LLaMA Model - BERTScore: {final_bertscore_llama:.4f}\")\n"
      ],
      "metadata": {
        "id": "jAA6pkkDUG3W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}